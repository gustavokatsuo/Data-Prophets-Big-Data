# M√≥dulo para tratamento e processamento de dados
import pandas as pd
import numpy as np
from tqdm import tqdm
from .config import LAG_FEATURES, OUTLIER_PARAMS, DUMMY_FEATURES, EMBEDDING_FEATURES, EMBEDDING_CONFIG
import warnings
import multiprocessing as mp
from functools import partial
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
warnings.filterwarnings('ignore')

def _process_outlier_group_wrapper(args):
    """Wrapper function para desempacotar argumentos do multiprocessing."""
    name_group_tuple, outlier_params = args
    name, group = name_group_tuple
    group.name = name
    return _process_outlier_group(group, **outlier_params)

def _process_outlier_group(group, rolling_window, z_threshold, treatment, adaptive_threshold, min_observations, absolute_cap_multiplier=5):
    """Fun√ß√£o auxiliar para processar outliers em um √∫nico grupo (para paraleliza√ß√£o)."""
    try:
        (pdv, product) = group.name
        n_obs = len(group)
        
        # Early exit para grupos muito pequenos
        if n_obs < 3:
            return group, {
                'pdv': pdv, 'product': product, 'n_obs': n_obs, 'mean_qty': 0,
                'std_qty': 0, 'cv': 0, 'zero_pct': 0,
                'threshold_used': z_threshold, 'n_outliers': 0, 'outlier_pct': 0
            }
        
        # Usar valores numpy para melhor performance
        qty_values = group['qty'].values
        
        # Calcular caracter√≠sticas b√°sicas do grupo usando numpy
        mean_qty = np.mean(qty_values)
        std_qty = np.std(qty_values) or 1.0
        zero_pct = np.mean(qty_values == 0)
        cv = std_qty / (mean_qty + 1e-8)

        threshold = z_threshold
        
        # L√≥gica de threshold adaptativo (otimizada)
        if adaptive_threshold:
            if n_obs < min_observations:
                threshold *= 1.3 if cv > 2.0 else (1.2 if zero_pct > 0.7 else 1.0)
            else:
                if cv > 2.0: threshold *= 1.5
                elif cv > 1.0: threshold *= 1.2
                else: threshold *= 0.8
                if zero_pct > 0.7: threshold *= 1.3
                if mean_qty > 100: threshold *= 0.9

        # Calcular outliers de forma otimizada
        if n_obs < min_observations:
            # M√©todo simples para grupos pequenos
            outliers_mask = np.abs(qty_values - mean_qty) > (threshold * std_qty)
        else:
            # M√©todo rolling otimizado usando pandas (mais confi√°vel)
            group_sorted = group.sort_values('week_of_year').copy()
            
            # Usar pandas rolling que √© mais robusto
            rolling_mean = group_sorted['qty'].shift(1).rolling(rolling_window, min_periods=3).mean().fillna(mean_qty)
            rolling_std = group_sorted['qty'].shift(1).rolling(rolling_window, min_periods=3).std().fillna(std_qty)
            rolling_std = rolling_std.replace(0, std_qty)
            
            # Calcular Z-scores
            z_scores = np.abs((group_sorted['qty'] - rolling_mean) / rolling_std)
            outliers_mask_sorted = z_scores > threshold
            
            # Calcular Z-scores (outliers_mask_sorted √© uma pd.Series com o √≠ndice ordenado)
            z_scores = np.abs((group_sorted['qty'] - rolling_mean) / rolling_std)
            outliers_mask_sorted = z_scores > threshold

            # Mapear de volta para a ordem original usando reindex (muito mais r√°pido)
            outliers_mask = outliers_mask_sorted.reindex(group.index).fillna(False).values

        n_outliers = np.sum(outliers_mask)
        
        group_stats = {
            'pdv': pdv, 'product': product, 'n_obs': n_obs, 'mean_qty': mean_qty,
            'std_qty': std_qty, 'cv': cv, 'zero_pct': zero_pct,
            'threshold_used': threshold, 'n_outliers': n_outliers,
            'outlier_pct': (n_outliers / n_obs) * 100 if n_obs > 0 else 0
        }

        # Aplicar tratamento apenas se houver outliers
        if n_outliers > 0:
            group_copy = group.copy()  # S√≥ copiar se necess√°rio
            
            # Aplicar cap absoluto primeiro (mais agressivo)
            absolute_cap = mean_qty * absolute_cap_multiplier
            extreme_mask = qty_values > absolute_cap
            if np.any(extreme_mask):
                group_copy.loc[extreme_mask, 'qty'] = absolute_cap
            
            if treatment == 'winsorize':
                # Usar percentis mais conservadores
                p02, p98 = np.percentile(qty_values, [2, 98])
                # Garantir que os limites s√£o razo√°veis
                p02 = max(0, p02)
                p98 = min(p98, mean_qty * 3)  # Cap adicional: m√°ximo 3x a m√©dia
                
                group_copy.loc[outliers_mask, 'qty'] = np.clip(group_copy.loc[outliers_mask, 'qty'], p02, p98)
            
            elif treatment == 'cap':
                # Calcular limites de forma mais conservadora
                if n_obs >= min_observations:
                    # Usar percentil 90 como limite superior mais seguro (era 95)
                    upper_limit = min(np.percentile(qty_values, 90), mean_qty + 1.5 * std_qty)
                    lower_limit = max(0, np.percentile(qty_values, 10))
                else:
                    upper_limit = mean_qty + 1.5 * std_qty
                    lower_limit = max(0, mean_qty - 1.5 * std_qty)
                
                # Aplicar cap apenas para reduzir outliers, nunca aumentar
                outlier_values = group_copy.loc[outliers_mask, 'qty']
                capped_values = np.where(outlier_values > upper_limit, upper_limit, 
                                       np.where(outlier_values < lower_limit, lower_limit, outlier_values))
                group_copy.loc[outliers_mask, 'qty'] = capped_values

            elif treatment == 'remove':
                group_copy = group_copy[~outliers_mask]
            
            return group_copy, group_stats
        else:
            return group, group_stats

    except Exception as e:
        # Retornar o grupo original e estat√≠sticas vazias em caso de erro
        return group, {}

class DataProcessor:
    """Classe para processamento e engenharia de features dos dados"""
    
    def __init__(self):
        self.df_raw = None
        self.df_agg = None
        self.feature_columns = None
        self.dummy_encoders = {}
        self.label_encoders = {}
        self.pca_transformers = {}
        self.categorical_features = []
        
    def load_data(self, file_path):
        """Carrega dados do arquivo parquet"""
        print("üìä Carregando dados...")
        self.df_raw = pd.read_parquet(file_path)
        self.df_raw['quantity'] = self.df_raw['quantity'].fillna(0)
        
        # Remover coluna semana_do_ano se existir e criar week_of_year baseado em datas
        self.df_raw = self.df_raw.drop(columns=['semana_do_ano'], errors='ignore')

        # Remove PDCs inv√°lidos
        self.df_raw = self.df_raw[self.df_raw['pdv'].notnull() & (self.df_raw['pdv'] != '') & (self.df_raw['pdv'] != 'desconhecido')]
        self.df_raw['pdv'] = self.df_raw['pdv'].astype(int)
        min_date = self.df_raw['transaction_date'].min()
        self.df_raw['week_of_year'] = (self.df_raw['transaction_date'] - min_date).dt.days // 7 + 1
        
        # GAMBIARRA ESPEC√çFICA: Remo√ß√£o da semana 37 (dados inflacionados ~60x)
        # Esta semana tem valores an√¥malos que n√£o representam vendas reais - REMOVER COMPLETAMENTE
        if 37 in self.df_raw['week_of_year'].values:
            print("‚ö†Ô∏è GAMBIARRA DETECTADA: Semana 37 encontrada com dados inflacionados!")
            print("   ‚Üí Aplicando corre√ß√£o RADICAL: REMOVENDO semana 37 e interpolando valores")
            
            week_37_records = (self.df_raw['week_of_year'] == 37).sum()
            
            # REMOVER COMPLETAMENTE todos os dados da semana 37
            self.df_raw = self.df_raw[self.df_raw['week_of_year'] != 37].copy()
            
            print(f"   ‚Üí Semana 37 REMOVIDA: {week_37_records:,} registros eliminados")
            print(f"   ‚Üí Interpola√ß√£o ser√° feita automaticamente durante a agrega√ß√£o")
        
        
        print(f"   ‚Üí Dados carregados: {len(self.df_raw):,} registros")
        return self.df_raw
    
    def aggregate_data(self):
        """Agrega dados por semana/PDV/produto"""
        print("üîÑ Agregando dados por semana/PDV/produto...")
        
        # Verificar quais colunas categ√≥ricas existem nos dados
        available_categorical = [col for col in ['categoria', 'premise', 'subcategoria', 'tipos', 'label', 'fabricante'] 
                               if col in self.df_raw.columns]
        
        print(f"   ‚Üí Colunas categ√≥ricas dispon√≠veis: {available_categorical}")
        
        # Preparar dicion√°rio de agrega√ß√£o
        agg_dict = {
            'quantity': 'sum',
            'gross_value': 'sum'
        }
        
        # Para colunas categ√≥ricas, usar 'first' (assumindo que s√£o constantes por PDV-produto)
        for col in available_categorical:
            agg_dict[col] = 'first'
        
        # Agrega√ß√£o principal
        self.df_agg = self.df_raw.groupby(['week_of_year', 'pdv', 'internal_product_id'], as_index=False).agg(agg_dict)
        
        # Renomear colunas principais
        self.df_agg = self.df_agg.rename(columns={'quantity': 'qty', 'gross_value': 'gross'})
        
        # INTERPOLA√á√ÉO: Preencher lacuna da semana 37 removida
        if 37 not in self.df_agg['week_of_year'].values and 36 in self.df_agg['week_of_year'].values and 38 in self.df_agg['week_of_year'].values:
            print("üîß INTERPOLA√á√ÉO: Preenchendo lacuna da semana 37...")
            self._interpolate_missing_week(37)
        
        print(f"   ‚Üí Dados agregados iniciais: {len(self.df_agg):,} registros")
        print(f"   ‚Üí Dados agregados finais: {len(self.df_agg):,} registros")
        print(f"   ‚Üí PDVs √∫nicos: {self.df_agg['pdv'].nunique():,}")
        print(f"   ‚Üí Produtos √∫nicos: {self.df_agg['internal_product_id'].nunique():,}")
        print(f"   ‚Üí Range de semanas: {self.df_agg['week_of_year'].min()}-{self.df_agg['week_of_year'].max()}")
        
        return self.df_agg
    
    def _interpolate_missing_week(self, missing_week):
        """Interpola dados para uma semana ausente usando m√©dia das semanas vizinhas"""
        print(f"   ‚Üí Interpolando dados para semana {missing_week}...")
        
        # Identificar semanas vizinhas
        prev_week = missing_week - 1
        next_week = missing_week + 1
        
        # Buscar dados das semanas vizinhas
        prev_data = self.df_agg[self.df_agg['week_of_year'] == prev_week].copy()
        next_data = self.df_agg[self.df_agg['week_of_year'] == next_week].copy()
        
        if prev_data.empty or next_data.empty:
            print(f"   ‚Üí Aviso: N√£o foi poss√≠vel interpolar semana {missing_week} - semanas vizinhas ausentes")
            return
        
        # Fazer merge das semanas vizinhas para interpolar
        interpolated = prev_data.merge(
            next_data[['pdv', 'internal_product_id', 'qty', 'gross']],
            on=['pdv', 'internal_product_id'],
            how='outer',
            suffixes=('_prev', '_next')
        )
        
        # Calcular valores interpolados (m√©dia simples)
        interpolated['qty'] = (
            interpolated['qty_prev'].fillna(0) + interpolated['qty_next'].fillna(0)
        ) / 2
        interpolated['gross'] = (
            interpolated['gross_prev'].fillna(0) + interpolated['gross_next'].fillna(0)
        ) / 2
        
        # Preparar DataFrame interpolado
        interpolated['week_of_year'] = missing_week
        
        # Manter colunas categ√≥ricas da semana anterior (ou pr√≥xima se anterior n√£o existe)
        categorical_cols = [col for col in interpolated.columns 
                          if col not in ['pdv', 'internal_product_id', 'week_of_year', 'qty', 'gross'] 
                          and not col.endswith(('_prev', '_next'))]
        
        for col in categorical_cols:
            if col in interpolated.columns:
                interpolated[col] = interpolated[col].fillna(method='ffill').fillna(method='bfill')
        
        # Selecionar colunas finais
        final_cols = ['week_of_year', 'pdv', 'internal_product_id', 'qty', 'gross'] + categorical_cols
        interpolated_final = interpolated[final_cols].copy()
        
        # Adicionar dados interpolados ao DataFrame principal
        self.df_agg = pd.concat([self.df_agg, interpolated_final], ignore_index=True)
        
        print(f"   ‚Üí Semana {missing_week} interpolada: {len(interpolated_final):,} registros adicionados")
    
    def create_lag_features(self):
        """Cria features de lag e rolling de forma vetorizada"""
        print("‚ö° Criando features com opera√ß√µes vetorizadas...")
        
        # Ordenar dados
        self.df_agg = self.df_agg.sort_values(['pdv', 'internal_product_id', 'week_of_year'])
        
        # Criar lags
        print("  ‚Üí Criando lags...")
        for lag in LAG_FEATURES:
            self.df_agg[f'lag_{lag}'] = self.df_agg.groupby(['pdv', 'internal_product_id'])['qty'].shift(lag)
        
        # Features rolling
        print("  ‚Üí Criando features rolling...")
        grouped = self.df_agg.groupby(['pdv', 'internal_product_id'])['qty']
        self.df_agg['rmean_4'] = grouped.shift(1).rolling(4, min_periods=1).mean()
        print("    ‚Üí M√©dia m√≥vel 4 criada")
        
        self.df_agg['rstd_4'] = grouped.shift(1).rolling(4, min_periods=1).std()
        print("    ‚Üí Desvio padr√£o m√≥vel 4 criado")
        
        # Otimiza√ß√£o para fra√ß√£o de n√£o zeros
        is_nonzero = (self.df_agg['qty'] > 0).astype('int8')
        self.df_agg['nonzero_frac_8'] = (
            is_nonzero.groupby([self.df_agg['pdv'], self.df_agg['internal_product_id']])
            .shift(1)
            .rolling(8, min_periods=1)
            .mean()
        )
        print("    ‚Üí Fra√ß√£o de n√£o zeros nas √∫ltimas 8 criada")
        
        # Preencher NAs
        lag_cols = [c for c in self.df_agg.columns if c.startswith('lag_')] + ['rmean_4', 'rstd_4', 'nonzero_frac_8']
        self.df_agg[lag_cols] = self.df_agg[lag_cols].fillna(0)
        
        # Criar features categ√≥ricas
        self.create_categorical_features()
        
        # Definir colunas de features b√°sicas (removido 'gross' para evitar data leakage)
        base_features = ['week_of_year'] + lag_cols
        self.feature_columns = base_features + self.categorical_features
        
        print(f"   ‚Üí {len(lag_cols)} features de lag criadas")
        print(f"   ‚Üí Total de features: {len(self.feature_columns)}")
        return self.df_agg
    
    def create_categorical_features(self):
        """Cria features categ√≥ricas usando dummiza√ß√£o e embeddings"""
        print("üé≠ Criando features categ√≥ricas...")
        self.categorical_features = []
        
        # Verificar se as colunas categ√≥ricas existem nos dados
        available_dummy = [col for col in DUMMY_FEATURES if col in self.df_agg.columns]
        available_embedding = [col for col in EMBEDDING_FEATURES if col in self.df_agg.columns]
        
        if not available_dummy and not available_embedding:
            print("   ‚Üí Nenhuma coluna categ√≥rica encontrada nos dados agregados")
            return
        
        # 1. Features Dummy (One-Hot Encoding)
        for col in available_dummy:
            print(f"  ‚Üí Criando dummies para {col}...")
            # Verificar valores √∫nicos e criar dummies manualmente
            unique_values = self.df_agg[col].value_counts().head(10).index.tolist()
            
            for value in unique_values:
                dummy_col = f'{col}_{value}'.replace(' ', '_').replace('-', '_')
                self.df_agg[dummy_col] = (self.df_agg[col] == value).astype('int8')
                self.categorical_features.append(dummy_col)
            
            print(f"    ‚Üí {len(unique_values)} dummies criadas para {col}")
        
        # 2. Features de Embedding (usando PCA)
        for col in available_embedding:
            print(f"  ‚Üí Criando embeddings para {col}...")
            
            # Frequ√™ncia m√≠nima para manter categorias
            value_counts = self.df_agg[col].value_counts()
            valid_categories = value_counts[value_counts >= EMBEDDING_CONFIG['min_frequency']].index
            
            # Substituir categorias raras por 'outros'
            self.df_agg[f'{col}_processed'] = self.df_agg[col].apply(
                lambda x: x if x in valid_categories else 'outros'
            )
            
            # Label Encoding
            le = LabelEncoder()
            encoded_values = le.fit_transform(self.df_agg[f'{col}_processed'].fillna('outros'))
            
            # Se h√° muitas categorias √∫nicas, usar PCA para reduzir dimensionalidade
            n_unique = len(le.classes_)
            if n_unique > EMBEDDING_CONFIG['max_unique_values']:
                print(f"    ‚Üí {n_unique} categorias, aplicando PCA...")
                
                # Criar matriz de embeddings simples (baseada em frequ√™ncia e co-ocorr√™ncia)
                embedding_matrix = self._create_simple_embeddings(self.df_agg[f'{col}_processed'], n_unique)
                
                # Aplicar PCA
                pca = PCA(n_components=min(EMBEDDING_CONFIG['n_components'], n_unique-1))
                reduced_embeddings = pca.fit_transform(embedding_matrix)
                
                # Mapear de volta para os dados
                for i in range(reduced_embeddings.shape[1]):
                    feature_name = f'{col}_emb_{i}'
                    self.df_agg[feature_name] = reduced_embeddings[encoded_values, i]
                    self.categorical_features.append(feature_name)
                
                # Salvar transformadores para predi√ß√µes futuras
                self.label_encoders[col] = le
                self.pca_transformers[col] = (pca, embedding_matrix)
                
                print(f"    ‚Üí {reduced_embeddings.shape[1]} componentes PCA criados para {col}")
            else:
                # Para poucas categorias, usar embeddings simples baseados em target encoding
                target_means = self.df_agg.groupby(f'{col}_processed')['qty'].mean()
                self.df_agg[f'{col}_target_enc'] = self.df_agg[f'{col}_processed'].map(target_means)
                self.categorical_features.append(f'{col}_target_enc')
                
                # Salvar encoder
                self.label_encoders[col] = target_means
                
                print(f"    ‚Üí Target encoding criado para {col}")
            
            # Remover coluna tempor√°ria
            self.df_agg.drop(f'{col}_processed', axis=1, inplace=True)
        
        print(f"   ‚Üí Total de {len(self.categorical_features)} features categ√≥ricas criadas")
    
    def _create_simple_embeddings(self, categorical_series, n_unique):
        """Cria embeddings simples baseados em frequ√™ncia e co-ocorr√™ncia"""
        # Matriz simples: cada linha √© uma categoria, colunas s√£o features estat√≠sticas
        embedding_dim = min(20, n_unique)
        embedding_matrix = np.zeros((n_unique, embedding_dim))
        
        value_counts = categorical_series.value_counts()
        unique_values = categorical_series.unique()
        
        for i, value in enumerate(unique_values):
            # Feature 1: Log da frequ√™ncia
            embedding_matrix[i, 0] = np.log1p(value_counts.get(value, 1))
            
            # Features adicionais: estat√≠sticas baseadas em posi√ß√£o e contexto
            if embedding_dim > 1:
                # Usar hash simples para criar features determin√≠sticas
                hash_val = hash(str(value)) % 1000000
                for j in range(1, min(embedding_dim, 10)):
                    embedding_matrix[i, j] = (hash_val % (j * 100 + 1)) / (j * 100 + 1)
        
        return embedding_matrix

    def detect_and_treat_outliers(self, rolling_window=12, z_threshold=5.0, treatment='winsorize', 
                                  adaptive_threshold=True, min_observations=10, absolute_cap_multiplier=3,
                                  method='median_imputation', percentile_cap=99.5, absolute_cap=220):
        """
        Detecta e trata outliers usando diferentes m√©todos
        
        Args:
            method (str): 'percentile_cap', 'z_score', 'median_imputation', ou 'hybrid'
            percentile_cap (float): Percentil para cap (ex: 95.0)
            absolute_cap (float): Cap absoluto como backup
            rolling_window (int): Tamanho da janela rolling para calcular m√©dia e desvio
            z_threshold (float): Threshold base do Z-score para considerar outlier
            treatment (str): Tipo de tratamento ('winsorize', 'remove', 'cap', 'median_impute')
            adaptive_threshold (bool): Se True, ajusta threshold baseado nas caracter√≠sticas do grupo
            min_observations (int): M√≠nimo de observa√ß√µes para calcular estat√≠sticas confi√°veis
            absolute_cap_multiplier (float): Cap absoluto como m√∫ltiplo da m√©dia do grupo (m√©todo antigo)
        """
        
        if method == 'median_imputation':
            return self._median_imputation_outliers(percentile_cap, absolute_cap)
        elif method == 'percentile_cap':
            return self._percentile_cap_outliers(percentile_cap, absolute_cap)
        else:
            return self._detect_and_treat_outliers(rolling_window, z_threshold, treatment, 
                                                               adaptive_threshold, min_observations, absolute_cap_multiplier)
    
    def _median_imputation_outliers(self, percentile_cap=95.0, absolute_cap=24, rolling_window=12):
        """
        M√©todo de imputa√ß√£o por mediana: substitui outliers pela mediana usando apenas dados passados
        """
        print(f"üéØ Aplicando imputacao por mediana para outliers...")
        print(f"   ‚Üí Metodo: Detectar outliers usando janela rolling de {rolling_window} semanas")
        print(f"   ‚Üí Percentil: {percentile_cap}% | Cap absoluto: {absolute_cap}")
        
        # Ordenar dados por PDV, produto e semana
        self.df_agg = self.df_agg.sort_values(['pdv', 'internal_product_id', 'week_of_year']).reset_index(drop=True)
        
        # Calcular estat√≠sticas antes
        total_records = len(self.df_agg)
        total_volume_before = self.df_agg['qty'].sum()
        
        print(f"   ‚Üí Processando {total_records:,} registros de forma vetorizada...")
        
        # Usar opera√ß√µes vetorizadas para calcular rolling statistics com shift=1 (sem data leakage)
        grouped = self.df_agg.groupby(['pdv', 'internal_product_id'])['qty']
        
        # Calcular estat√≠sticas rolling usando apenas dados passados (shift=1)
        rolling_median = grouped.shift(1).rolling(rolling_window, min_periods=3).median()
        rolling_percentile = grouped.shift(1).rolling(rolling_window, min_periods=3).quantile(percentile_cap / 100)
        
        # Aplicar cap absoluto e percentil (usar o menor)
        outlier_threshold = np.minimum(rolling_percentile.fillna(absolute_cap), absolute_cap)
        
        # Identificar outliers de forma vetorizada
        outliers_mask = (self.df_agg['qty'] > outlier_threshold) & (outlier_threshold.notna())
        
        # Contar outliers
        outliers_count = outliers_mask.sum()
        outliers_volume = self.df_agg.loc[outliers_mask, 'qty'].sum()
        
        # Imputar outliers com a mediana rolling (apenas dados passados)
        self.df_agg.loc[outliers_mask, 'qty'] = rolling_median.loc[outliers_mask]
        
        # Para casos onde rolling_median √© NaN, usar valor conservador (m√≠nimo entre 1 e valor original)
        still_nan_mask = outliers_mask & self.df_agg['qty'].isna()
        if still_nan_mask.any():
            self.df_agg.loc[still_nan_mask, 'qty'] = 1.0  # Valor conservador para casos extremos
        
        # Estat√≠sticas ap√≥s tratamento
        total_volume_after = self.df_agg['qty'].sum()
        volume_reduction = ((total_volume_before - total_volume_after) / total_volume_before) * 100
        
        # Calcular estat√≠sticas para o relat√≥rio
        final_median = self.df_agg['qty'].median()
        final_max = self.df_agg['qty'].max()
        final_mean = self.df_agg['qty'].mean()
        
        print(f"\nüìä RESULTADO DA IMPUTA√á√ÉO POR MEDIANA:")
        print(f"   ‚Üí Registros afetados: {outliers_count:,} ({outliers_count/total_records*100:.3f}%)")
        print(f"   ‚Üí Volume antes: {total_volume_before:,.0f}")
        print(f"   ‚Üí Volume depois: {total_volume_after:,.0f}")
        print(f"   ‚Üí Redu√ß√£o de volume: {volume_reduction:.1f}%")
        print(f"   ‚Üí Novo m√°ximo: {final_max:.1f}")
        print(f"   ‚Üí Nova m√©dia: {final_mean:.2f}")
        print(f"   ‚Üí Nova mediana: {final_median:.1f}")
        
        # Criar estat√≠sticas para compatibilidade
        self.outlier_stats = pd.DataFrame({
            'method': ['median_imputation_no_leakage'],
            'total_outliers': [outliers_count],
            'volume_reduction_pct': [volume_reduction],
            'threshold_used': [absolute_cap],  # Cap absoluto como refer√™ncia
            'final_median': [final_median]
        })
        
        print("‚úÖ Imputa√ß√£o por mediana conclu√≠da!")
        return self.df_agg
    
    def _percentile_cap_outliers(self, percentile_cap=99.5, absolute_cap=220, rolling_window=12):
        """
        M√©todo mais robusto OTIMIZADO: cap baseado em percentil usando apenas dados passados
        """
        print(f"üéØ Aplicando tratamento robusto de outliers...")
        print(f"   ‚Üí Metodo: Cap baseado em percentil {percentile_cap}% + cap absoluto {absolute_cap}")
        print(f"   ‚Üí Janela rolling: {rolling_window} semanas")
        
        # Ordenar dados por PDV, produto e semana
        self.df_agg = self.df_agg.sort_values(['pdv', 'internal_product_id', 'week_of_year']).reset_index(drop=True)
        
        # Calcular estat√≠sticas antes
        total_records = len(self.df_agg)
        total_volume_before = self.df_agg['qty'].sum()
        
        print(f"   ‚Üí Processando {total_records:,} registros de forma vetorizada...")
        
        # Usar opera√ß√µes vetorizadas para calcular rolling statistics com shift=1
        grouped = self.df_agg.groupby(['pdv', 'internal_product_id'])['qty']
        
        # Calcular cap rolling usando apenas dados passados (shift=1)
        rolling_percentile = grouped.shift(1).rolling(rolling_window, min_periods=3).quantile(percentile_cap / 100)
        
        # Aplicar cap absoluto e percentil (usar o menor)
        final_cap = np.minimum(rolling_percentile.fillna(absolute_cap), absolute_cap)
        
        # Identificar outliers de forma vetorizada
        outliers_mask = (self.df_agg['qty'] > final_cap) & (final_cap.notna())
        
        # Contar outliers
        outliers_count = outliers_mask.sum()
        outliers_volume = (self.df_agg.loc[outliers_mask, 'qty'] - final_cap.loc[outliers_mask]).sum()
        
        # Aplicar cap de forma vetorizada
        self.df_agg.loc[outliers_mask, 'qty'] = final_cap.loc[outliers_mask]
        
        # Estat√≠sticas ap√≥s tratamento
        total_volume_after = self.df_agg['qty'].sum()
        volume_reduction = ((total_volume_before - total_volume_after) / total_volume_before) * 100
        
        # Calcular estat√≠sticas finais
        final_max = self.df_agg['qty'].max()
        final_mean = self.df_agg['qty'].mean()
        
        print(f"\nüìä RESULTADO DO TRATAMENTO ROBUSTO:")
        print(f"   ‚Üí Registros afetados: {outliers_count:,} ({outliers_count/total_records*100:.3f}%)")
        print(f"   ‚Üí Volume antes: {total_volume_before:,.0f}")
        print(f"   ‚Üí Volume depois: {total_volume_after:,.0f}")
        print(f"   ‚Üí Redu√ß√£o de volume: {volume_reduction:.1f}%")
        print(f"   ‚Üí Novo m√°ximo: {final_max:.1f}")
        print(f"   ‚Üí Nova m√©dia: {final_mean:.2f}")
        
        # Criar estat√≠sticas para compatibilidade
        self.outlier_stats = pd.DataFrame({
            'method': ['percentile_cap_no_leakage'],
            'total_outliers': [outliers_count],
            'volume_reduction_pct': [volume_reduction],
            'final_cap': [absolute_cap]  # Cap absoluto como refer√™ncia
        })
        
        print("‚úÖ Tratamento robusto de outliers conclu√≠do!")
        return self.df_agg

    def _detect_and_treat_outliers(self, rolling_window=12, z_threshold=5.0, 
                                               treatment='winsorize', adaptive_threshold=True, 
                                               min_observations=10, absolute_cap_multiplier=3.0):
        """
        Vers√£o otimizada para detec√ß√£o e tratamento de outliers por PDV-produto.
        """
        print(f"üéØ Detectando outliers com processamento paralelo...")
        print(f"   ‚Üí Janela rolling: {rolling_window} semanas | Z-score base: {z_threshold}")
        start_time = pd.Timestamp.now()
        
        self.df_agg = self.df_agg.sort_values(['pdv', 'internal_product_id', 'week_of_year'])
        
        # Criar lista de grupos para processamento paralelo
        groups_list = [(name, group) for name, group in self.df_agg.groupby(['pdv', 'internal_product_id'], sort=False)]
        n_groups = len(groups_list)
        print(f"  ‚Üí Processando {n_groups:,} combina√ß√µes PDV-produto em paralelo...")

        # Par√¢metros para outliers
        outlier_params = {
            'rolling_window': rolling_window,
            'z_threshold': z_threshold,
            'treatment': treatment,
            'adaptive_threshold': adaptive_threshold,
            'min_observations': min_observations,
            'absolute_cap_multiplier': absolute_cap_multiplier
        }

        # Criar argumentos para a fun√ß√£o wrapper
        args_list = [(group_tuple, outlier_params) for group_tuple in groups_list]

        # Determinar n√∫mero de processos otimizado
        num_processes = min(mp.cpu_count(), n_groups)  # N√£o criar mais processos que grupos
        print(f"  ‚Üí Utilizando {num_processes} processos...")

        # Usar chunksize otimizado para melhor balanceamento
        chunksize = max(1, n_groups // (num_processes * 4))  # 4 chunks por processo
        
        results = []
        # Usar Pool para processamento paralelo com barra de progresso
        with mp.Pool(processes=num_processes) as pool:
            with tqdm(total=n_groups, desc="Analisando grupos PDV-produto") as pbar:
                # Usar imap com chunksize para melhor performance
                for result in pool.imap(_process_outlier_group_wrapper, args_list, chunksize=chunksize):
                    results.append(result)
                    pbar.update()

        # Coletar resultados de forma mais eficiente
        processed_groups = []
        group_stats = []
        
        for res in results:
            if res and len(res) == 2:
                group_data, stats = res
                if group_data is not None and not group_data.empty:
                    processed_groups.append(group_data)
                if stats:
                    group_stats.append(stats)

        # Reconstruir o DataFrame de forma otimizada
        if processed_groups:
            # Usar concat com ignore_index=True e sort=False para melhor performance
            self.df_agg = pd.concat(processed_groups, ignore_index=True, sort=False)
        
        # Criar DataFrame de estat√≠sticas de forma mais eficiente
        if group_stats:
            stats_df = pd.DataFrame(group_stats)
            total_outliers = stats_df['n_outliers'].sum()
        else:
            stats_df = pd.DataFrame()
            total_outliers = 0
        
        # Estat√≠sticas finais
        end_time = pd.Timestamp.now()
        duration = (end_time - start_time).total_seconds()
        
        # A contagem de outlier_pct pode ser imprecisa se 'remove' for usado
        # pois o len(self.df_agg) mudou. Calculamos antes da remo√ß√£o.
        initial_records = sum(stats_df['n_obs']) if not stats_df.empty else 1
        outlier_pct = (total_outliers / initial_records) * 100 if initial_records > 0 else 0
        
        print(f"\nüìä RELAT√ìRIO DE OUTLIERS PERSONALIZADOS:")
        print(f"   ‚Üí Total de outliers: {total_outliers:,} ({outlier_pct:.2f}% dos dados originais)")
        print(f"   ‚Üí Grupos processados: {len(stats_df):,}")
        print(f"   ‚Üí Tratamento aplicado: {treatment}")
        
        if not stats_df.empty:
            print(f"\nüìà ESTAT√çSTICAS POR GRUPO:")
            print(f"   ‚Üí Threshold m√©dio usado: {stats_df['threshold_used'].mean():.2f}")
            print(f"   ‚Üí Grupos com outliers: {(stats_df['n_outliers'] > 0).sum():,}")
            print(f"   ‚Üí CV m√©dio: {stats_df['cv'].mean():.2f}")
            print(f"   ‚Üí % zero m√©dia: {stats_df['zero_pct'].mean()*100:.1f}%")
            
            top_outliers = stats_df.nlargest(5, 'outlier_pct')
            if not top_outliers.empty:
                print(f"\nüéØ TOP 5 GRUPOS COM MAIS OUTLIERS:")
                for _, row in top_outliers.iterrows():
                    print(f"   ‚Üí PDV {row['pdv']}, Produto {row['product']}: "
                          f"{row['n_outliers']} outliers ({row['outlier_pct']:.1f}%)")
        
        print(f"\n‚úÖ Processamento paralelo conclu√≠do!")
        print(f"   ‚Üí Tempo total: {duration:.2f} segundos")
        print(f"   ‚Üí Dados finais: {len(self.df_agg):,} registros")
        
        self.outlier_stats = stats_df
        
        return self.df_agg
    
    def analyze_outlier_treatment(self):
        """
        Analisa os resultados do tratamento personalizado de outliers
        Retorna insights sobre os padr√µes encontrados
        """
        if not hasattr(self, 'outlier_stats') or self.outlier_stats is None:
            print("‚ö†Ô∏è Nenhuma estat√≠stica de outliers dispon√≠vel. Execute detect_and_treat_outliers primeiro.")
            return None
        
        stats = self.outlier_stats
        
        # Verificar se o DataFrame est√° vazio ou n√£o tem as colunas necess√°rias
        if stats.empty:
            print("üìä AN√ÅLISE DETALHADA DO TRATAMENTO DE OUTLIERS:")
            print("="*60)
            print("üéØ RESUMO GERAL:")
            print("   ‚Üí Nenhum grupo foi processado ou nenhuma estat√≠stica foi coletada.")
            print("   ‚Üí Verifique se os dados de entrada est√£o corretos.")
            return {
                'total_groups': 0,
                'groups_with_outliers': 0,
                'total_outliers': 0,
                'stats_by_pdv': None,
                'stats_by_product': None,
                'threshold_stats': None,
                'detailed_stats': stats
            }
        
        # Verificar se √© o novo m√©todo median_imputation
        if 'method' in stats.columns and not stats.empty:
            method_name = stats['method'].iloc[0]
            
            if method_name in ['median_imputation']:
                print("üìä AN√ÅLISE DETALHADA DO TRATAMENTO DE OUTLIERS:")
                print("="*60)
                print(f"‚úÖ M√©todo aplicado: {method_name}")
                print(f"   ‚Üí Total de outliers tratados: {stats['total_outliers'].iloc[0]:,}")
                print(f"   ‚Üí Redu√ß√£o de volume: {stats['volume_reduction_pct'].iloc[0]:.1f}%")
                
                if 'final_median' in stats.columns:
                    print(f"   ‚Üí Mediana final: {stats['final_median'].iloc[0]:.1f}")
                elif 'median_value' in stats.columns:
                    print(f"   ‚Üí Valor da mediana: {stats['median_value'].iloc[0]:.1f}")
                    
                print("   ‚Üí Tratamento por imputa√ß√£o com mediana - muito conservador!")
                return {
                    'total_groups': 1,
                    'groups_with_outliers': 1 if stats['total_outliers'].iloc[0] > 0 else 0,
                    'total_outliers': stats['total_outliers'].iloc[0],
                    'method': method_name,
                    'detailed_stats': stats
                }
            
            elif method_name in ['percentile_cap']:
                print("üìä AN√ÅLISE DETALHADA DO TRATAMENTO DE OUTLIERS:")
                print("="*60)
                print(f"‚úÖ M√©todo aplicado: {method_name}")
                print(f"   ‚Üí Total de outliers tratados: {stats['total_outliers'].iloc[0]:,}")
                print(f"   ‚Üí Redu√ß√£o de volume: {stats['volume_reduction_pct'].iloc[0]:.1f}%")
                print(f"   ‚Üí Cap aplicado: {stats['final_cap'].iloc[0]:.0f}")
                    
                print("   ‚Üí Tratamento baseado em percentil - mais robusto!")
                return {
                    'total_groups': 1,
                    'groups_with_outliers': 1 if stats['total_outliers'].iloc[0] > 0 else 0,
                    'total_outliers': stats['total_outliers'].iloc[0],
                    'method': method_name,
                    'detailed_stats': stats
                }
        
        # Verificar se as colunas necess√°rias existem (m√©todo antigo)
        required_cols = ['n_outliers', 'pdv', 'product', 'cv', 'zero_pct', 'mean_qty', 'threshold_used']
        missing_cols = [col for col in required_cols if col not in stats.columns]
        if missing_cols:
            print("üìä AN√ÅLISE DETALHADA DO TRATAMENTO DE OUTLIERS:")
            print("="*60)
            print(f"‚ö†Ô∏è Colunas necess√°rias n√£o encontradas: {missing_cols}")
            print("   ‚Üí O processamento pode ter falhado. Verifique os logs anteriores.")
            return {
                'total_groups': len(stats),
                'groups_with_outliers': 0,
                'total_outliers': 0,
                'stats_by_pdv': None,
                'stats_by_product': None,
                'threshold_stats': None,
                'detailed_stats': stats
            }
        
        print("üìä AN√ÅLISE DETALHADA DO TRATAMENTO DE OUTLIERS:")
        print("="*60)
        
        # An√°lise geral
        total_groups = len(stats)
        groups_with_outliers = (stats['n_outliers'] > 0).sum()
        
        print(f"üéØ RESUMO GERAL:")
        print(f"   ‚Üí Total de grupos PDV-produto: {total_groups:,}")
        print(f"   ‚Üí Grupos com outliers: {groups_with_outliers:,} ({groups_with_outliers/total_groups*100:.1f}%)")
        print(f"   ‚Üí Total de outliers encontrados: {stats['n_outliers'].sum():,}")
        
        # An√°lise por caracter√≠sticas dos produtos
        print(f"\nüìà AN√ÅLISE POR CARACTER√çSTICAS:")
        
        # Produtos por faixa de variabilidade (CV)
        cv_stats = stats.dropna(subset=['cv'])
        if len(cv_stats) > 0:
            print(f"   ‚Üí Coeficiente de Varia√ß√£o:")
            print(f"     ‚Ä¢ Baixa variabilidade (CV < 1.0): {(cv_stats['cv'] < 1.0).sum():,} grupos")
            print(f"     ‚Ä¢ M√©dia variabilidade (1.0 ‚â§ CV < 2.0): {((cv_stats['cv'] >= 1.0) & (cv_stats['cv'] < 2.0)).sum():,} grupos")
            print(f"     ‚Ä¢ Alta variabilidade (CV ‚â• 2.0): {(cv_stats['cv'] >= 2.0).sum():,} grupos")
        
        # Produtos por % de zeros
        print(f"   ‚Üí Produtos com muitos zeros:")
        print(f"     ‚Ä¢ At√© 30% zeros: {(stats['zero_pct'] <= 0.3).sum():,} grupos")
        print(f"     ‚Ä¢ 30-70% zeros: {((stats['zero_pct'] > 0.3) & (stats['zero_pct'] <= 0.7)).sum():,} grupos")
        print(f"     ‚Ä¢ Mais de 70% zeros: {(stats['zero_pct'] > 0.7).sum():,} grupos")
        
        # Produtos por volume m√©dio
        print(f"   ‚Üí Produtos por volume m√©dio:")
        print(f"     ‚Ä¢ Baixo volume (< 10): {(stats['mean_qty'] < 10).sum():,} grupos")
        print(f"     ‚Ä¢ M√©dio volume (10-100): {((stats['mean_qty'] >= 10) & (stats['mean_qty'] < 100)).sum():,} grupos")
        print(f"     ‚Ä¢ Alto volume (‚â• 100): {(stats['mean_qty'] >= 100).sum():,} grupos")
        
        # Top PDVs com mais outliers (apenas se h√° outliers)
        if groups_with_outliers > 0:
            print(f"\nüè™ TOP 10 PDVs COM MAIS OUTLIERS:")
            pdv_outliers = stats.groupby('pdv').agg({
                'n_outliers': 'sum',
                'product': 'count'
            }).sort_values('n_outliers', ascending=False).head(10)
            
            for pdv, row in pdv_outliers.iterrows():
                if row['n_outliers'] > 0:  # S√≥ mostrar PDVs que realmente t√™m outliers
                    print(f"   ‚Üí PDV {pdv}: {row['n_outliers']:,} outliers em {row['product']:,} produtos")
            
            # Top produtos com mais outliers
            print(f"\nüì¶ TOP 10 PRODUTOS COM MAIS OUTLIERS:")
            product_outliers = stats.groupby('product').agg({
                'n_outliers': 'sum',
                'pdv': 'count'
            }).sort_values('n_outliers', ascending=False).head(10)
            
            for product, row in product_outliers.iterrows():
                if row['n_outliers'] > 0:  # S√≥ mostrar produtos que realmente t√™m outliers
                    print(f"   ‚Üí Produto {product}: {row['n_outliers']:,} outliers em {row['pdv']:,} PDVs")
        else:
            print(f"\n‚úÖ NENHUM OUTLIER DETECTADO:")
            print(f"   ‚Üí Todos os {total_groups:,} grupos PDV-produto est√£o dentro dos par√¢metros normais")
            print(f"   ‚Üí Os dados parecem estar bem comportados ou os thresholds s√£o muito altos")
            pdv_outliers = None
            product_outliers = None
        
        # Thresholds utilizados
        print(f"\nüéõÔ∏è THRESHOLDS ADAPTATIVOS UTILIZADOS:")
        threshold_stats = stats['threshold_used'].describe()
        print(f"   ‚Üí M√≠nimo: {threshold_stats['min']:.2f}")
        print(f"   ‚Üí M√©dio: {threshold_stats['mean']:.2f}")
        print(f"   ‚Üí M√°ximo: {threshold_stats['max']:.2f}")
        print(f"   ‚Üí Desvio padr√£o: {threshold_stats['std']:.2f}")
        
        return {
            'total_groups': total_groups,
            'groups_with_outliers': groups_with_outliers,
            'total_outliers': stats['n_outliers'].sum(),
            'stats_by_pdv': pdv_outliers,
            'stats_by_product': product_outliers,
            'threshold_stats': threshold_stats,
            'detailed_stats': stats
        }
    
    def split_train_validation(self, train_cutoff=44, valid_start=45, valid_end=48):
        """Divide dados em treino e valida√ß√£o"""
        print("üìà Preparando conjuntos de treino e valida√ß√£o...")
        
        train = self.df_agg[self.df_agg['week_of_year'] <= train_cutoff]
        valid = self.df_agg[(self.df_agg['week_of_year'] >= valid_start) & 
                           (self.df_agg['week_of_year'] <= valid_end)]
        
        print(f"   ‚Üí Treino: {len(train):,} registros (semanas <= {train_cutoff})")
        print(f"   ‚Üí Valida√ß√£o: {len(valid):,} registros (semanas {valid_start}-{valid_end})")
        
        return train, valid
    
    def get_basic_stats(self):
        """Retorna estat√≠sticas b√°sicas dos dados"""
        if self.df_agg is None:
            return None
            
        zero_pct = (self.df_agg['qty'] == 0).mean() * 100
        
        stats = {
            'total_records': len(self.df_agg),
            'unique_pdvs': self.df_agg['pdv'].nunique(),
            'unique_products': self.df_agg['internal_product_id'].nunique(),
            'mean_sales': self.df_agg['qty'].mean(),
            'median_sales': self.df_agg['qty'].median(),
            'zero_percentage': zero_pct,
            'mean_gross_value': self.df_agg['gross'].mean(),
            'weeks_range': (self.df_agg['week_of_year'].min(), self.df_agg['week_of_year'].max())
        }
        
        return stats
    
    def prepare_features_target(self, data, target_col='qty'):
        """Prepara features e target para modelagem"""
        if self.feature_columns is None:
            raise ValueError("Features n√£o foram criadas. Execute create_lag_features() primeiro.")
            
        X = data[self.feature_columns]
        y = data[target_col]
        
        return X, y

def process_data(file_path, treat_outliers=True, outlier_params=None):
    """
    Fun√ß√£o principal para processar dados completos
    
    Args:
        file_path (str): Caminho para o arquivo de dados
        treat_outliers (bool): Se deve aplicar tratamento de outliers
        outlier_params (dict): Par√¢metros para tratamento de outliers personalizado
    """
    if outlier_params is None:
        outlier_params = OUTLIER_PARAMS
    
    processor = DataProcessor()
    
    # Pipeline de processamento
    processor.load_data(file_path)
    processor.aggregate_data()
    
    # Salvar dados antes do tratamento de outliers para compara√ß√£o
    df_before_outliers = processor.df_agg.copy() if treat_outliers else None
    
    # Tratamento de outliers personalizado
    if treat_outliers:
        processor.detect_and_treat_outliers(**outlier_params)
        # An√°lise dos resultados
        outlier_analysis = processor.analyze_outlier_treatment()
        
        # Gerar gr√°ficos de compara√ß√£o antes/depois do tratamento
        from .visualization import DataVisualizer
        visualizer = DataVisualizer()
        print("\nüé® Gerando visualiza√ß√µes de compara√ß√£o do tratamento de outliers...")
        outlier_stats = getattr(processor, 'outlier_stats', None)
        visualizer.plot_outlier_treatment_comparison(
            df_before_outliers, 
            processor.df_agg, 
            outlier_stats=outlier_stats,
            save_plots=True
        )
    else:
        outlier_analysis = None
    
    processor.create_lag_features()
    
    # Dividir dados
    train_data, valid_data = processor.split_train_validation()
    
    # Preparar features e targets
    X_train, y_train = processor.prepare_features_target(train_data)
    X_val, y_val = processor.prepare_features_target(valid_data)
    
    # Estat√≠sticas
    stats = processor.get_basic_stats()
    
    return {
        'processor': processor,
        'raw_data': processor.df_raw,
        'aggregated_data': processor.df_agg,
        'train_data': (X_train, y_train),
        'validation_data': (X_val, y_val),
        'feature_columns': processor.feature_columns,
        'stats': stats,
        'outlier_analysis': outlier_analysis
    }