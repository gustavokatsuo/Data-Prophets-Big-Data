# Pipeline principal - orquestra todo o workflow
import os
import sys
import argparse
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Imports dos m√≥dulos criados
from scripts.config import *
from scripts.data_processing import process_data
from scripts.visualization import create_visualizations
from scripts.model import train_model, generate_predictions
from scripts.xgboost_model import train_xgboost_model, generate_xgboost_predictions

def main(model_type='lightgbm'):
    """Pipeline principal do projeto"""
    print("üöÄ INICIANDO PIPELINE DE PREDI√á√ÉO DE VENDAS")
    print("=" * 60)
    print(f"‚è±Ô∏è  Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ü§ñ Modelo selecionado: {model_type.upper()}")
    print(f"üíª Usando {cpu_count()} cores para processamento paralelo")
    print()
    
    # ===== ETAPA 1: PROCESSAMENTO DE DADOS =====
    print("üìä ETAPA 1: PROCESSAMENTO DE DADOS")
    print("-" * 40)
    
    try:
        data_dict = process_data(DATA_PATH)
        print("‚úÖ Dados processados com sucesso!")
    except Exception as e:
        print(f"‚ùå Erro no processamento de dados: {e}")
        return None
    
    print()
    
    # ===== ETAPA 2: AN√ÅLISE VISUAL E INSIGHTS =====
    print("üìà ETAPA 2: AN√ÅLISE VISUAL E INSIGHTS")
    print("-" * 40)
    
    try:
        viz_results = create_visualizations(data_dict, output_dir=OUTPUT_DIR)
        print("‚úÖ An√°lises visuais geradas com sucesso!")
    except Exception as e:
        print(f"‚ùå Erro na gera√ß√£o de visualiza√ß√µes: {e}")
        viz_results = {}
    
    print()
    
    # ===== ETAPA 3: TREINAMENTO DO MODELO =====
    print(f"ü§ñ ETAPA 3: TREINAMENTO DO MODELO {model_type.upper()}")
    print("-" * 40)
    
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if model_type.lower() == 'xgboost':
            model_path = os.path.join(MODELS_DIR, f'xgboost_model_{timestamp}.pkl')
            model, wmape = train_xgboost_model(data_dict, save_model_path=model_path)
            print("‚úÖ Modelo XGBoost treinado com sucesso!")
        else:  # lightgbm (padr√£o)
            model_path = os.path.join(MODELS_DIR, f'lightgbm_model_{timestamp}.pkl')
            model, wmape = train_model(data_dict, save_model_path=model_path)
            print("‚úÖ Modelo LightGBM treinado com sucesso!")
            
        print(f"üìä WMAPE de valida√ß√£o: {wmape:.4f}")
    except Exception as e:
        print(f"‚ùå Erro no treinamento do modelo: {e}")
        return None
    
    print()
    
    # ===== ETAPA 4: AN√ÅLISE DO MODELO =====
    print("üéØ ETAPA 4: AN√ÅLISE DO MODELO")
    print("-" * 40)
    
    try:
        model_viz_results = create_visualizations(
            data_dict, 
            model=model, 
            output_dir=OUTPUT_DIR
        )
        
        # Salvar feature importance
        if 'feature_importance' in model_viz_results:
            importance_path = os.path.join(OUTPUT_DIR, 'feature_importance.csv')
            model_viz_results['feature_importance'].to_csv(importance_path, index=False)
            print(f"üíæ Feature importance salva em: {importance_path}")
        
        print("‚úÖ An√°lise do modelo conclu√≠da!")
    except Exception as e:
        print(f"‚ùå Erro na an√°lise do modelo: {e}")
    
    print()
    
    # ===== ETAPA 5: GERA√á√ÉO DE PREDI√á√ïES =====
    print("üîÆ ETAPA 5: GERA√á√ÉO DE PREDI√á√ïES")
    print("-" * 40)
    
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if model_type.lower() == 'xgboost':
            predictions_path = os.path.join(OUTPUT_DIR, f'submission_xgboost_jan2023_{timestamp}.csv')
            predictions_df = generate_xgboost_predictions(
                model, 
                data_dict, 
                weeks=PREDICTION_WEEKS,
                parallel=True,
                save_path=predictions_path
            )
            print("‚úÖ Predi√ß√µes XGBoost geradas com sucesso!")
        else:  # lightgbm (padr√£o)
            predictions_path = os.path.join(OUTPUT_DIR, f'submission_lightgbm_jan2023_{timestamp}.csv')
            predictions_df = generate_predictions(
                model, 
                data_dict, 
                weeks=PREDICTION_WEEKS,
                parallel=True,
                save_path=predictions_path
            )
            print("‚úÖ Predi√ß√µes LightGBM geradas com sucesso!")
            
    except Exception as e:
        print(f"‚ùå Erro na gera√ß√£o de predi√ß√µes: {e}")
        return None
    
    print()
    
    # ===== ETAPA 6: AN√ÅLISE DAS PREDI√á√ïES =====
    print("üìä ETAPA 6: AN√ÅLISE DAS PREDI√á√ïES")
    print("-" * 40)
    
    try:
        pred_viz_results = create_visualizations(
            data_dict,
            predictions_df=predictions_df,
            output_dir=OUTPUT_DIR
        )
        print("‚úÖ An√°lise das predi√ß√µes conclu√≠da!")
    except Exception as e:
        print(f"‚ùå Erro na an√°lise das predi√ß√µes: {e}")
    
    print()
    
    # ===== RELAT√ìRIO FINAL =====
    print("‚úÖ PIPELINE CONCLU√çDO COM SUCESSO!")
    print("=" * 60)
    print(f"‚è±Ô∏è  Finalizado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìÅ Resultados salvos em: {OUTPUT_DIR}/")
    print(f"ü§ñ Modelo salvo em: {MODELS_DIR}/")
    print()
    
    # Estat√≠sticas finais
    print("üìà RESUMO DOS RESULTADOS:")
    print(f"‚Ä¢ Modelo usado: {model_type.upper()}")
    print(f"‚Ä¢ Total de predi√ß√µes: {len(predictions_df):,}")
    print(f"‚Ä¢ PDVs √∫nicos: {predictions_df['pdv'].nunique():,}")
    print(f"‚Ä¢ Produtos √∫nicos: {predictions_df['produto'].nunique():,}")
    print(f"‚Ä¢ WMAPE do modelo: {wmape:.4f}")
    
    # Estat√≠sticas por semana
    pred_stats = predictions_df.groupby('semana')['quantidade'].sum()
    print("\nüîÆ PREDI√á√ïES POR SEMANA:")
    for week in PREDICTION_WEEKS:
        if week in pred_stats.index:
            print(f"  Semana {week}: {pred_stats[week]:>10,} unidades")
    
    print()
    print("üéâ Processo finalizado! Verifique os arquivos de output para mais detalhes.")
    
    return {
        'data': data_dict,
        'model': model,
        'predictions': predictions_df,
        'wmape': wmape
    }

def run_quick_analysis():
    """Execu√ß√£o r√°pida apenas com an√°lise de dados (sem modelo)"""
    print("üöÄ AN√ÅLISE R√ÅPIDA DOS DADOS")
    print("=" * 40)
    
    # Processar dados
    data_dict = process_data(DATA_PATH)
    
    # Gerar visualiza√ß√µes
    create_visualizations(data_dict, output_dir=OUTPUT_DIR)
    
    print("‚úÖ An√°lise r√°pida conclu√≠da!")
    return data_dict

if __name__ == "__main__":
    # Configurar parser de argumentos
    parser = argparse.ArgumentParser(description='Pipeline de Predi√ß√£o de Vendas')
    parser.add_argument('--model', 
                       choices=['lightgbm', 'xgboost'], 
                       default='lightgbm',
                       help='Tipo de modelo a ser usado (lightgbm ou xgboost)')
    parser.add_argument('--quick', 
                       action='store_true',
                       help='Execu√ß√£o r√°pida apenas com an√°lise de dados')
    
    args = parser.parse_args()
    
    # Executar pipeline baseado nos argumentos
    if args.quick:
        run_quick_analysis()
    else:
        main(model_type=args.model)  